---
title: "Joppa"
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r DoNotModify, include=FALSE}
### Utilities. Do not modify.
# Installation of packages if necessary
InstallPackages <- function(Packages) {
  InstallPackage <- function(Package) {
    if (!Package %in% installed.packages()[, 1]) {
      install.packages(Package, repos="https://cran.rstudio.com/")
    }
  }
  invisible(sapply(Packages, InstallPackage))
}
# Basic packages
InstallPackages(c("rmarkdown", "formatR", "kableExtra", "ragg"))
library("kableExtra") # Mandatory to load css and more
# Chunk font size hook: allows size='small' or any valid Latex font size in chunk options
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r Options, include=FALSE}
### Customized options for this document
# Add necessary packages here
Packages <- c("tidyverse", "entropart")
# Install them
InstallPackages(Packages)
# knitr options
knitr::opts_chunk$set(
  cache = TRUE, # Cache chunk results
  echo = TRUE, # Show R chunks
  warning=FALSE, # Hide warnings
  # Books only: figures with side captions
  # fig.env='SCfigure', fig.asp=.75,
  # Figure alignment and size
  fig.align='center', out.width='80%',
  # Graphic device
  dev = "ragg_png",
  # Code chunk format
  tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=80),
  size="scriptsize", knitr.graphics.auto_pdf = TRUE
  )
options(width=80)
# ggplot style
library("tidyverse")
theme_set(theme_bw())
theme_update(panel.background=element_rect(fill="transparent", colour=NA),
             plot.background=element_rect(fill="transparent", colour=NA))
knitr::opts_chunk$set(dev.args=list(bg="transparent"))
# Random seed
set.seed(973)
```

# Données

```{r}
data <- read.csv2("data/Lombrics.csv")
# data <- data[-(1:13), ]
data <- data[data$S>0, ] 
```

Les données sont :

-  Le nombre d'unités de temps passé depuis le début de la période d'étude $Y_i=i$,
```{r}
Y <- data$Y
```

- Le nombre d'espèces découvertes $S_i$ à chaque unité de temps, 
```{r}
S <- data$S
```

- Le nombre cumulé d'espèces découvertes, qui augmente à chaque unité de temps,
```{r}
# Valeur initiale
S_0 <- 1 # L. terrestris par Linné, 1758
```

- Le nombre de taxonomistes actifs $T_i$ à chaque unité de temps, 
```{r}
T <- data$T
```


# Inférence

Le logarithme du nombre d'espèces découvertes à chaque unité de temps est donné par le modèle 
$$\log{S_i} = \log{\left( S_T \beta_1 T_i + \beta_2 S_T T_i Y_i + \beta_1 T_i \sum{S_i} - \beta_2 T_i Y_i \sum{S_i} \right)} + \mathcal{N}\left(0, \sigma \right)$$.

La variable expliquée est $S$.
Le nombre d'espèces décrites au début de la période d'étude est $S_0$.

La variable explicative est $T$ : le nombre de taxonomistes actifs chaque année.

Les paramètres sont $\theta = (\beta_1, \beta_2, S_T, \sigma)$.


## Vraisemblance

La vraisemblance des données `S` sachant les paramètres est calculée :
```{r}
# Arguments:
# S : nombre d'espèces découvertes au temps i
# T : nombre de taxonomistes 
# Y : année (temps)
# S_0 : nombres d'espèces connues au début
# theta : paramètres
ll_S_theta <- function(S, T, Y, S_0, theta) {
  nTemps <- length(Y)
  # Stockage des prédictions du modèle et de leur cumul
  prediction <- numeric(nTemps)
  sum_S <- numeric(nTemps+1)
  # Valeur initiale
  sum_S[1] <- S_0
  # Valeurs prédites par le modèle
  for (i in 1:nTemps) {
    prediction[i] <- (theta[3] * theta[1] * T[i]
            + theta[2] * theta[3] * T[i] * Y[i]
            + theta[1] * T[i] * sum_S[i]
            + theta[2] * T[i] * Y[i] * sum_S[i])
    sum_S[i + 1] <- sum_S[i] + prediction[i]
  }
  # Log-vraisemblance de chaque valeur de y
  likelihoods <- suppressWarnings(dnorm(log(S), mean = log(prediction), sd = theta[4], log = TRUE))
  # Possibilité de NaN 
  ll <- sum(likelihoods)
  # Vraisemblance nulle dans ce cas
  if (is.na(ll)) 
    ll <- -Inf
  return(ll)
}
```

La vraisemblance des paramètres est:
```{r}
ll_prior <- function(theta, S_0) {
    # Log-vraisemblance de chaque paramètre dans sa loi a priori
    prior1 <- dunif(theta[1], min = 0, max = 1, log = TRUE)      # beta_1
    prior2 <- dunif(theta[2], min = 0, max = 1, log = TRUE)      # beta_2
    prior3 <- dunif(theta[3], min = S_0, max = 1E5, log = TRUE)  # S_T
    prior4 <- dunif(theta[4], min = 0, max = 1000, log = TRUE)   # sigma
    return(prior1 + prior2 + prior3 + prior4)
}
```


## Parcours de l'espace des paramètres

Fonction de proposition :
```{r}
proposal <- function(theta, sigma_prop) {
    return(rnorm(length(theta), mean = theta, sd = sigma_prop))
}
```

Chaîne de Markov :
```{r} 
MetropolisMCMC <- function(S, T, Y, S_0, sigma_prop, theta_0, iterations) {
    # Stockage. Chaque ligne du tableau contient une itération ; les colonnes contiennent theta
    chain <- matrix(nrow = iterations + 1, ncol = length(theta_0))
    # Noms des paramètres
    colnames(chain) <- names(theta_0)
    # Stockage des vraisemblances (1: ll_S_theta, 2: ll_prior)
    ll_data <- matrix(nrow = iterations + 1, ncol = 2)
    colnames(ll_data) <- c("data", "prior")
    # Valeurs initiales
    chain[1, ] <- theta_0
    ll_data[1, 1] <- ll_S_theta(S, T, Y, S_0, theta_0)
    ll_data[1, 2] <- ll_prior(theta_0, S_0)
    # Initialisation d'une barre de progression
    pgb <- txtProgressBar(min = 0, max = iterations)
    # Chaîne de Markov
    for (i in 1:iterations) {
        # Proposition d'une valeur de theta
        theta_proposal <- proposal(chain[i, ], sigma_prop)
        # Vraisemblances
        ll_data[i + 1, 1] <- ll_S_theta(S, T, Y, S_0, theta_proposal)
        ll_data[i + 1, 2] <- ll_prior(theta_proposal, S_0)
        # Acceptation
        if (sum(ll_data[i, ]) == -Inf) {
          # Acceptation systématique si la vraisemblance précédente était nulle
          l_ratio <- Inf
        } else {
        # Rapport de vraisemblance entre la proposition et la valeur précédente de theta
          l_ratio <- exp(sum(ll_data[i + 1, ]) - sum(ll_data[i, ]))
        }
        # Acceptation ou non
        if (runif(1) < l_ratio) {
            chain[i + 1, ] <- theta_proposal
        } else {
            chain[i + 1, ] <- chain[i, ]
            # Conserver les valeurs de vraisemblance
            ll_data[i + 1, ] <- ll_data[i, ]
        }
        setTxtProgressBar(pgb, i)
    }
    # Retour d'un tableau complet: theta et vraisemblance
    return(cbind(chain, LogVraisemblance = ll_data))
}
```


## Exécution

```{r}
# Nombre de pas de la chaîne
iterations <- 1e+7
# Ecart-type de la marche aléatoire (pour chaque paramètre)
sigma_prop <- c(1E-3, 1E-5, 1E4, 1)/10
# Valeur initiale des paramètres
theta_0 <- c(
  1E-2,  # beta_1
  1E-6,  # beta_2
  1E4,   # S_T
  1     # sigma
  )
names(theta_0) <- c("beta_1", "beta_2", "S_T", "EcartType")
# Lancement de la chaîne de Markov
chain <- MetropolisMCMC(S, T, Y, S_0, sigma_prop, theta_0, iterations)
```


## Résultats

```{r}
# Evolution de la vraisemblance
par(mfrow = c(1, 2))
plot(chain[, length(theta_0) + 1], type = "l", main = "Evolution de la vraisemblance", xlab = "Pas", ylab = "log Vraisemblance")
burn_in <- iterations/10
plot(chain[-(1:burn_in), length(theta_0) + 1], type = "l", main = "Après convergence", xlab = "Pas", ylab = "log Vraisemblance")
```

Evolution des paramètres:
```{r}
par(mfrow = c(2, 2))
plot(chain[-(1:burn_in), 1], type = "l", main = "", xlab = "beta_1", ylab = "")
plot(chain[-(1:burn_in), 2], type = "l", main = "", xlab = "beta_2", ylab = "")
plot(chain[-(1:burn_in), 3], type = "l", main = "", xlab = "S_T", ylab = "")
plot(chain[-(1:burn_in), 4], type = "l", main = "", xlab = "sigma", ylab = "")
```


## Taux d'acceptation

```{r}
# Taux d'acceptation de la chaine de Markov
(acceptance <- 1 - mean(duplicated(chain[-(1:burn_in), ])))
```

## Distribution des paramètres

```{r}
par(mfrow = c(1, length(theta_0)))
for (i in 1:length(theta_0)) {
    Titre <- paste("Distribution de", names(theta_0)[i])
    Mediane <- format(median(chain[-(1:burn_in), i]), digits = 4)
    hist(chain[-(1:burn_in), i], xlab = paste("Mediane :", Mediane), main = Titre)
    abline(v = Mediane, col = "red")
}
```


## Autocorrélation

```{r}
acf(chain[-(1:burn_in), 1:(ncol(chain)-2)], lag.max = 1000)
```

Éclaircie:
```{r}
# Distribution a posteriori. Elimination du prechauffage et de la vraisemblance, éclaircie.
posterior <- chain[-(1:burn_in), 1:(ncol(chain)-2)][seq(1, 
    iterations - burn_in, by = 5000), ]
# Nouvelle distribution des paramètres
par(mfrow = c(1, length(theta_0)))
for (i in 1:length(theta_0)) {
    Titre <- paste("Distribution de", names(theta_0)[i])
    Mediane <- format(median(posterior[, i]), digits = 4)
    hist(posterior[, i], xlab = paste("Mediane :", Mediane), main = Titre)
    abline(v = Mediane, col = "red")
}
```

Nouvelle autocorrélation:
```{r}
acf(posterior, lag.max = 100)
```


# Nombre d'espèces estimé

```{r}
library("tidyverse")
library("entropart")
(S_est <- median(posterior[, 3]))
as.SimTest(S_est, posterior[, 3]) %>% autoplot
```

# Prédictions

```{r}
# Graphique du nombre cumulé d'espèces observé (data) et prédit (paramètres du modèle)
Ajustement <- function(data, beta_1, beta_2, S_T, sigma) {
  # Stockage des S_i
  S <- numeric(length(data$Y))
  sum_S <- numeric(length(data$Y) + 1)
  sum_S[1] <- S_0
  # Calcul de S_i à chaque temps
  for (i in seq_along(data$Y)) {
    S[i] <- exp(log(S_T*beta_1*T[i] 
                      + beta_2*S_T*T[i]*Y[i] 
                      + beta_1*T[i]*sum_S[i] 
                      + beta_2*T[i]*Y[i]*sum_S[i]) 
                  + rnorm(1, sd=sigma))
    # Cumul des espèces découvertes
    sum_S[i+1] <- sum_S[i] + S[i]
  }
  # Figure
  tibble(`Année`=c(1817, 1817+5*data$Y), `Estimé`=sum_S, `Observé`=c(S_0, cumsum(data$S)+S_0)) %>% 
    pivot_longer(cols = c("Estimé", "Observé")) %>% 
    rename(`Nb espèces`=name, `Nombre`=value) %>% 
    ggplot() +
      geom_line(aes(x=`Année`, y=`Nombre`, color=`Nb espèces`)) +
      scale_y_log10() %>% 
    return
}
```

Qualité de prédiction du modèle estimé: les paramètres sont les médianes des distributions.
```{r}
# Médiane
beta_1 <- median(posterior[, 1])
beta_2 <- median(posterior[, 2])
S_T <-    median(posterior[, 3])
sigma <-  median(posterior[, 4])
Ajustement(data, beta_1, beta_2, S_T, sigma)

```

La robustesse de l'estimation est évaluée en prenant les valeurs extrêmes du nombre d'espèces estimées.

```{r}
# Nombre d'espèces maximal
iteration <- which.max(posterior[, 3])
beta_1 <- posterior[iteration, 1]
beta_2 <- posterior[iteration, 2]
(S_T <-    posterior[iteration, 3])
sigma <-  posterior[iteration, 4]
Ajustement(data, beta_1, beta_2, S_T, sigma)
```

```{r}
# Nombre d'espèces minimal
iteration <- which.min(posterior[, 3])
beta_1 <- posterior[iteration, 1]
beta_2 <- posterior[iteration, 2]
(S_T <-    posterior[iteration, 3])
sigma <-  posterior[iteration, 4]
Ajustement(data, beta_1, beta_2, S_T, sigma)
```

L'ajustement du modèle aux données est bon dans tous les cas.

L'estimation du nombre total d'espèces est corrélée négativement aux paramètres d'efficacité des taxonomistes: le modèle est donc peu discriminant.

```{r}
cor(posterior)
```
